from build_model import BartNewModelfrom train_bart_xsum import *def torch_cov(X):    D = X.shape[-1]    mean = torch.mean(X, dim=-1).unsqueeze(-1)    X = X - mean    return 1 / (D - 1) * X @ X.transpose(-1, -2)def T_dot(vecs):    result = None    for i in vecs:        i = i.unsqueeze(dim=0)        a = i.t() @ i        result = result + a if result is not None else a    return resultdef compute_kernel_bias(vecs=None,  # 新输入的向量集                        account=None,  # 迭代的数量n                        mean_u=None,  # 前边数据的均值                        W_already=None,  # 前边已经计算出来的W                        ):    # 计算kernel和bias    # vecs [num_samples, embedding_size]，    # 最后的变换：y = (x + bias).dot(kernel)    # vecs = np.concatenate(vecs, axis=0)    if account is None and mean_u is None and W_already is None:        # 从头计算        mu = torch.mean(vecs, dim=0)        cov = torch_cov(vecs.T)        u, s, vh = torch.svd(cov)        # W = np.dot(u, np.diag(s ** 0.5))        W = u @ torch.diag(s ** 0.5)        # W = np.linalg.inv(W.T)        W = torch.inverse(W.transpose(0, 1))        num = vecs.shape[0]    else:        # 需要将已有的mean_u、W_already和新输入的vecs一起迭代计算出新u和新W        # 新增的向量集条数        num = vecs.shape[0]        # 新的均值        mu = (mean_u * account + vecs.sum(axis=0, keepdims=True)) / (account + num)        if len(mean_u.shape) == 1:            mean_u = mean_u.unsqueeze(dim=0)        # 新的W        W = (account * (W_already + mean_u.t() @ mean_u) + T_dot(vecs)) / (account + num) - mu.t() @ mu        num = account + num    return W, mu, numdef transform_and_normalize(vecs, kernel=None, bias=None):    """应用变换，然后标准化    """    if not (kernel is None or bias is None):        vecs = (vecs + bias).dot(kernel)    return vecs / (vecs ** 2).sum(axis=1, keepdims=True) ** 0.5def shift_tokens_right(input_ids, pad_token_id, decoder_start_token_id):    """    Shift input ids one token to the right.    """    shifted_input_ids = input_ids.new_zeros(input_ids.shape)    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()    shifted_input_ids[:, 0] = decoder_start_token_id    assert pad_token_id is not None, "self.model.config.pad_token_id has to be defined."    # replace possible -100 values in labels by `pad_token_id`    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)    return shifted_input_idsif __name__ == "__main__":    # todo 返回模型的编码器的输出结果和解码器的输出结果，并且进行拼接    model = BartNewModel.from_pretrained("tst-summarization_xsum")    tokenizer = BartTokenizer.from_pretrained("tst-summarization_xsum")    processed_datasets = torch.load('data_cache/xsum/train_val.pt')    train_dataset = processed_datasets["train"]    accelerator = Accelerator()    data_collator = DataCollatorForSeq2Seq(        tokenizer,        model=model,        label_pad_token_id=-100,        pad_to_multiple_of=None,    )    train_dataloader = DataLoader(        train_dataset, shuffle=False, collate_fn=data_collator, batch_size=300    )    model = accelerator.prepare(        model    )    model.eval()    kernel_e, bias_e, turn_num_e = None, None, None    kernel_d, bias_d, turn_num_d = None, None, None    for step, batch in enumerate(tqdm(train_dataloader)):        with torch.no_grad():            input_ids = batch["input_ids"].cuda()            attention_mask = batch["attention_mask"].cuda()            decoder_input_ids = shift_tokens_right(                batch["labels"].cuda(), 1, 2            )            outputs = model(input_ids=input_ids,                            attention_mask=attention_mask,                            decoder_input_ids=decoder_input_ids,                            use_cache=False                            )            result_e = outputs.encoder_last_hidden_state.view(-1, 1024)            result_d = outputs.last_hidden_state.view(-1, 1024)            kernel_e, bias_e, turn_num_e = compute_kernel_bias(result_e, account=turn_num_e, mean_u=bias_e,                                                               W_already=kernel_e)            kernel_d, bias_d, turn_num_d = compute_kernel_bias(result_d, account=turn_num_d, mean_u=bias_d,                                                               W_already=kernel_d)            # outputs.cpu()            result_e.cpu()            result_d.cpu()            torch.cuda.empty_cache()    res = dict()    res["encoder_kernel"] = kernel_e    res["encoder_bias"] = bias_e    res["decoder_kernel"] = kernel_d    res["decoder_bias"] = bias_d    torch.save(res, "xsum_rep.pt")
