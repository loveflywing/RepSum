# 该文件主要用于生成和测试import nltkimport numpy as npfrom accelerate import Acceleratorfrom train_bart_xsum import parse_argsfrom torch.utils.data.dataloader import DataLoaderfrom tqdm import tqdmfrom build_model import *from transformers import DataCollatorForSeq2Seqfrom datasets import load_metric# def postprocess_text(preds, sums):#     preds = [pred.strip() for pred in preds]#     sums = [label.strip() for label in sums]##     # rougeLSum expects newline after each sentence#     preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]#     sums = ["\n".join(nltk.sent_tokenize(label)) for label in sums]##     return preds, sumsdef main():    # 读取验证集    processed_datasets = torch.load("./data_cache/xsum/train_val.pt")    test_dataset = processed_datasets["val"]    args = parse_args()    # tokenizer = BartTokenizer.from_pretrained('./tst-summarization')    # model = BartForSum.from_pretrained("./tst-summarization")    tokenizer = BartTokenizer.from_pretrained('./tst-summarization_xsum')    model = BartForSum.from_pretrained("./tst-summarization_xsum")    label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id    accelerator = Accelerator()    data_collator = DataCollatorForSeq2Seq(        tokenizer,        model=model,        label_pad_token_id=label_pad_token_id,        pad_to_multiple_of=8 if accelerator.use_fp16 else None,    )    test_dataloader = DataLoader(        test_dataset, shuffle=False, collate_fn=data_collator, batch_size=20    )    model, test_dataloader = accelerator.prepare(        model, test_dataloader    )    model.cuda()    model.eval()    gen_kwargs = {        "max_length": 50,        "num_beams": 6,    }    # metric = load_metric("rouge")    predictions = []    references = []    for step, batch in enumerate(tqdm(test_dataloader)):        with torch.no_grad():            generated_tokens = accelerator.unwrap_model(model).generate(                input_ids=batch["input_ids"],                attention_mask=batch["attention_mask"],                **gen_kwargs,            )            generated_tokens = accelerator.pad_across_processes(                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id            )            labels = batch["labels"]            labels = accelerator.pad_across_processes(batch["labels"], dim=1,                                                      pad_index=tokenizer.pad_token_id)            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()            labels = accelerator.gather(labels).cpu().numpy()            if args.ignore_pad_token_for_loss:                # Replace -100 in the labels as we can't decode them.                labels = np.where(labels != -100, labels, tokenizer.pad_token_id)            if isinstance(generated_tokens, tuple):                generated_tokens = generated_tokens[0]            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)            # 将生成的数据缓存进predictions，并在完成后写入txt文件            predictions.extend([pred.strip() for pred in decoded_preds])            references.extend([label.strip() for label in decoded_labels])            # decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)            # print(decoded_preds)            # metric.add_batch(predictions=decoded_preds, references=decoded_labels)    with open("result/xsum/only_decoder/prediction.txt", "w", encoding='utf-8') as f:        f.write("\n".join(predictions))    with open("result/xsum/only_decoder/label.txt", "w", encoding='utf-8') as f:        f.write("\n".join(references))    #    # result = metric.compute(use_stemmer=True)    # # Extract a few results from ROUGE    # result = {key: value.mid.fmeasure * 100 for key, value in result.items()}    #    # result = {k: round(v, 4) for k, v in result.items()}    #    # logger.info(result)if __name__ == '__main__':    main()